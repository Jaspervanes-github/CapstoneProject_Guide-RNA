{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1050 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56888 entries, 0 to 56887\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   seq     56888 non-null  object \n",
      " 1   y       56888 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 889.0+ KB\n",
      "None\n",
      "                     seq         y\n",
      "0  AAAAAAAAACTCCAAAACCCT  0.093147\n",
      "1  AAAAAACAACAAGAAGCACAA  0.064951\n",
      "2  AAAAAACACAAGCAAGACCGT  0.061797\n",
      "3  AAAAAACAGATGCCACCTGTG  0.057246\n",
      "4  AAAAAACCCGTAGATAGCCTC  0.067596\n"
     ]
    }
   ],
   "source": [
    "modelpath = \"zhihan1996/DNABERT-2-117M\"\n",
    "df = pd.read_csv(\"./sample_data/esp_decoded.csv\")\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "sequences = df['seq']\n",
    "labels = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlMElEQVR4nO3dfXRU9YH/8U8eyCRSZsLDZiazRojaCqlUKqlxROhac4glusuWbmVJkW0j1Jp0C1ExFA34GBqfUUoWtQ3nFBZ0j7CU0Eg2LGSFGDCSFQNEXaDgshP0QGYAJQ/k/v7w5P4YiUpwkmG+vF/nzDnNvd+5873fqvM+NzM3MZZlWQIAADBMbKQnAAAA0BeIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGio/0BCKpq6tLhw8f1qBBgxQTExPp6QAAgHNgWZaOHz8ur9er2Ngvvl5zUUfO4cOHlZaWFulpAACA83Do0CFdeumlX7j/oo6cQYMGSfpskZxOZ4RnAwAAzkUwGFRaWpr9Pv5FLurI6f4VldPpJHIAAIgyX/VREz54DAAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAI8VHegIA0FdGFFdGegq9dmBRbqSnABiDKzkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIvY6c2tpa3XbbbfJ6vYqJidHatWtD9luWpZKSEqWmpiopKUnZ2dl6//33Q8YcPXpUeXl5cjqdSk5OVn5+vk6cOBEy5p133tH48eOVmJiotLQ0lZWVnTWXV199VSNHjlRiYqJGjx6tDRs29PZ0AACAoXodOSdPntQ111yjJUuW9Li/rKxMixcvVnl5uerr6zVw4EDl5OTo1KlT9pi8vDw1NTWpurpa69evV21trWbNmmXvDwaDmjhxooYPH66GhgY98cQTWrhwoZYtW2aP2bZtm/7xH/9R+fn52rlzpyZPnqzJkyfr3Xff7e0pAQAAA8VYlmWd95NjYrRmzRpNnjxZ0mdXcbxer+655x7de++9kqRAICC3262KigpNnTpVe/bsUUZGhnbs2KHMzExJUlVVlSZNmqQPP/xQXq9XS5cu1fz58+X3+5WQkCBJKi4u1tq1a7V3715J0u23366TJ09q/fr19nyuv/56jRkzRuXl5ec0/2AwKJfLpUAgIKfTeb7LAOACNaK4MtJT6LUDi3IjPQXggneu799h/UzO/v375ff7lZ2dbW9zuVzKyspSXV2dJKmurk7Jycl24EhSdna2YmNjVV9fb4+ZMGGCHTiSlJOTo+bmZh07dswec+brdI/pfp2etLW1KRgMhjwAAICZwho5fr9fkuR2u0O2u91ue5/f71dKSkrI/vj4eA0ZMiRkTE/HOPM1vmhM9/6elJaWyuVy2Y+0tLTeniIAAIgSF9W3q+bNm6dAIGA/Dh06FOkpAQCAPhLWyPF4PJKklpaWkO0tLS32Po/HoyNHjoTs7+zs1NGjR0PG9HSMM1/ji8Z07++Jw+GQ0+kMeQAAADOFNXLS09Pl8XhUU1NjbwsGg6qvr5fP55Mk+Xw+tba2qqGhwR6zadMmdXV1KSsryx5TW1urjo4Oe0x1dbWuuuoqDR482B5z5ut0j+l+HQAAcHHrdeScOHFCjY2NamxslPTZh40bGxt18OBBxcTEaPbs2Xr00Ue1bt067dq1S3fccYe8Xq/9DaxRo0bplltu0cyZM7V9+3Zt3bpVhYWFmjp1qrxeryRp2rRpSkhIUH5+vpqamrR69Wo999xzKioqsufx61//WlVVVXrqqae0d+9eLVy4UG+99ZYKCwu//qoAAICoF9/bJ7z11lu66aab7J+7w2PGjBmqqKjQ3LlzdfLkSc2aNUutra268cYbVVVVpcTERPs5K1asUGFhoW6++WbFxsZqypQpWrx4sb3f5XJp48aNKigo0NixYzVs2DCVlJSE3Evnhhtu0MqVK/XAAw/oN7/5jb75zW9q7dq1uvrqq89rIQAAgFm+1n1yoh33yQHMxn1yADNF5D45AAAAFwoiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEbq9R/oBAD0Hf7eFhA+RA6AcxKNb74ALm78ugoAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkcIeOadPn9aDDz6o9PR0JSUl6YorrtAjjzwiy7LsMZZlqaSkRKmpqUpKSlJ2drbef//9kOMcPXpUeXl5cjqdSk5OVn5+vk6cOBEy5p133tH48eOVmJiotLQ0lZWVhft0AABAlAp75Pz2t7/V0qVL9cILL2jPnj367W9/q7KyMj3//PP2mLKyMi1evFjl5eWqr6/XwIEDlZOTo1OnTtlj8vLy1NTUpOrqaq1fv161tbWaNWuWvT8YDGrixIkaPny4Ghoa9MQTT2jhwoVatmxZuE8JAABEoRjrzEssYXDrrbfK7Xbr5ZdftrdNmTJFSUlJ+uMf/yjLsuT1enXPPffo3nvvlSQFAgG53W5VVFRo6tSp2rNnjzIyMrRjxw5lZmZKkqqqqjRp0iR9+OGH8nq9Wrp0qebPny+/36+EhARJUnFxsdauXau9e/ee01yDwaBcLpcCgYCcTmc4lwEwzojiykhPAReoA4tyIz0FXGTO9f077FdybrjhBtXU1Oi9996TJP33f/+33njjDf3whz+UJO3fv19+v1/Z2dn2c1wul7KyslRXVydJqqurU3Jysh04kpSdna3Y2FjV19fbYyZMmGAHjiTl5OSoublZx44d63FubW1tCgaDIQ8AAGCm+HAfsLi4WMFgUCNHjlRcXJxOnz6txx57THl5eZIkv98vSXK73SHPc7vd9j6/36+UlJTQicbHa8iQISFj0tPTzzpG977BgwefNbfS0lI99NBDYThLAABwoQv7lZxXXnlFK1as0MqVK/X2229r+fLlevLJJ7V8+fJwv1SvzZs3T4FAwH4cOnQo0lMCAAB9JOxXcu677z4VFxdr6tSpkqTRo0frL3/5i0pLSzVjxgx5PB5JUktLi1JTU+3ntbS0aMyYMZIkj8ejI0eOhBy3s7NTR48etZ/v8XjU0tISMqb75+4xn+dwOORwOL7+SQIAgAte2K/kfPLJJ4qNDT1sXFycurq6JEnp6enyeDyqqamx9weDQdXX18vn80mSfD6fWltb1dDQYI/ZtGmTurq6lJWVZY+pra1VR0eHPaa6ulpXXXVVj7+qAgAAF5ewR85tt92mxx57TJWVlTpw4IDWrFmjp59+Wn//938vSYqJidHs2bP16KOPat26ddq1a5fuuOMOeb1eTZ48WZI0atQo3XLLLZo5c6a2b9+urVu3qrCwUFOnTpXX65UkTZs2TQkJCcrPz1dTU5NWr16t5557TkVFReE+JQAAEIXC/uuq559/Xg8++KDuvvtuHTlyRF6vV7/4xS9UUlJij5k7d65OnjypWbNmqbW1VTfeeKOqqqqUmJhoj1mxYoUKCwt18803KzY2VlOmTNHixYvt/S6XSxs3blRBQYHGjh2rYcOGqaSkJOReOgAA4OIV9vvkRBPukwOcO+6Tgy/CfXLQ3yJ2nxwAAIALAZEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMFB/pCQAXmxHFlZGeAgBcFLiSAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwUp9Ezv/+7//qpz/9qYYOHaqkpCSNHj1ab731lr3fsiyVlJQoNTVVSUlJys7O1vvvvx9yjKNHjyovL09Op1PJycnKz8/XiRMnQsa88847Gj9+vBITE5WWlqaysrK+OB0AABCFwh45x44d07hx4zRgwAD9+c9/1u7du/XUU09p8ODB9piysjItXrxY5eXlqq+v18CBA5WTk6NTp07ZY/Ly8tTU1KTq6mqtX79etbW1mjVrlr0/GAxq4sSJGj58uBoaGvTEE09o4cKFWrZsWbhPCQAARKEYy7KscB6wuLhYW7du1X/913/1uN+yLHm9Xt1zzz269957JUmBQEBut1sVFRWaOnWq9uzZo4yMDO3YsUOZmZmSpKqqKk2aNEkffvihvF6vli5dqvnz58vv9yshIcF+7bVr12rv3r3nNNdgMCiXy6VAICCn0xmGswe+2ojiykhPAQirA4tyIz0FXGTO9f077Fdy1q1bp8zMTP3DP/yDUlJS9N3vflcvvviivX///v3y+/3Kzs62t7lcLmVlZamurk6SVFdXp+TkZDtwJCk7O1uxsbGqr6+3x0yYMMEOHEnKyclRc3Ozjh07Fu7TAgAAUSbskbNv3z4tXbpU3/zmN/X666/rl7/8pf75n/9Zy5cvlyT5/X5JktvtDnme2+229/n9fqWkpITsj4+P15AhQ0LG9HSMM1/j89ra2hQMBkMeAADATPHhPmBXV5cyMzP1+OOPS5K++93v6t1331V5eblmzJgR7pfrldLSUj300EMRnQMAAOgfYb+Sk5qaqoyMjJBto0aN0sGDByVJHo9HktTS0hIypqWlxd7n8Xh05MiRkP2dnZ06evRoyJiejnHma3zevHnzFAgE7MehQ4fO5xQBAEAUCHvkjBs3Ts3NzSHb3nvvPQ0fPlySlJ6eLo/Ho5qaGnt/MBhUfX29fD6fJMnn86m1tVUNDQ32mE2bNqmrq0tZWVn2mNraWnV0dNhjqqurddVVV4V8k+tMDodDTqcz5AEAAMwU9siZM2eO3nzzTT3++OP64IMPtHLlSi1btkwFBQWSpJiYGM2ePVuPPvqo1q1bp127dumOO+6Q1+vV5MmTJX125eeWW27RzJkztX37dm3dulWFhYWaOnWqvF6vJGnatGlKSEhQfn6+mpqatHr1aj333HMqKioK9ykBAIAoFPbP5Hzve9/TmjVrNG/ePD388MNKT0/Xs88+q7y8PHvM3LlzdfLkSc2aNUutra268cYbVVVVpcTERHvMihUrVFhYqJtvvlmxsbGaMmWKFi9ebO93uVzauHGjCgoKNHbsWA0bNkwlJSUh99IBAAAXr7DfJyeacJ8cRAL3yYFpuE8O+tu5vn+H/UoOAODiEo3hTphdHPgDnQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwUnykJwB8HSOKKyM9BQDABYorOQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADBSfKQngAvHiOLKSE8BAICw4UoOAAAwUp9HzqJFixQTE6PZs2fb206dOqWCggINHTpU3/jGNzRlyhS1tLSEPO/gwYPKzc3VJZdcopSUFN13333q7OwMGbN582Zde+21cjgcuvLKK1VRUdHXpwMAAKJEn0bOjh079C//8i/6zne+E7J9zpw5+tOf/qRXX31VW7Zs0eHDh/WjH/3I3n/69Gnl5uaqvb1d27Zt0/Lly1VRUaGSkhJ7zP79+5Wbm6ubbrpJjY2Nmj17tu688069/vrrfXlKAAAgSvRZ5Jw4cUJ5eXl68cUXNXjwYHt7IBDQyy+/rKefflo/+MEPNHbsWP3hD3/Qtm3b9Oabb0qSNm7cqN27d+uPf/yjxowZox/+8Id65JFHtGTJErW3t0uSysvLlZ6erqeeekqjRo1SYWGhfvzjH+uZZ57pq1MCAABRpM8ip6CgQLm5ucrOzg7Z3tDQoI6OjpDtI0eO1GWXXaa6ujpJUl1dnUaPHi23222PycnJUTAYVFNTkz3m88fOycmxjwEAAC5uffLtqlWrVuntt9/Wjh07ztrn9/uVkJCg5OTkkO1ut1t+v98ec2bgdO/v3vdlY4LBoD799FMlJSWd9dptbW1qa2uzfw4Gg70/OQAAEBXCfiXn0KFD+vWvf60VK1YoMTEx3If/WkpLS+VyuexHWlpapKcEAAD6SNgjp6GhQUeOHNG1116r+Ph4xcfHa8uWLVq8eLHi4+PldrvV3t6u1tbWkOe1tLTI4/FIkjwez1nftur++avGOJ3OHq/iSNK8efMUCATsx6FDh8JxygAA4AIU9si5+eabtWvXLjU2NtqPzMxM5eXl2f97wIABqqmpsZ/T3NysgwcPyufzSZJ8Pp927dqlI0eO2GOqq6vldDqVkZFhjznzGN1juo/RE4fDIafTGfIAAABmCvtncgYNGqSrr746ZNvAgQM1dOhQe3t+fr6Kioo0ZMgQOZ1O/epXv5LP59P1118vSZo4caIyMjI0ffp0lZWVye/364EHHlBBQYEcDock6a677tILL7yguXPn6uc//7k2bdqkV155RZWV3LUXAABE6M86PPPMM4qNjdWUKVPU1tamnJwc/e53v7P3x8XFaf369frlL38pn8+ngQMHasaMGXr44YftMenp6aqsrNScOXP03HPP6dJLL9VLL72knJycSJwSAAC4wMRYlmVFehKREgwG5XK5FAgE+NWV+NtVAC4eBxblRnoK+BrO9f2bv10FAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADBSfKQnYKoRxZWRngIAABc1ruQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIxE5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADBS2COntLRU3/ve9zRo0CClpKRo8uTJam5uDhlz6tQpFRQUaOjQofrGN76hKVOmqKWlJWTMwYMHlZubq0suuUQpKSm677771NnZGTJm8+bNuvbaa+VwOHTllVeqoqIi3KcDAACiVNgjZ8uWLSooKNCbb76p6upqdXR0aOLEiTp58qQ9Zs6cOfrTn/6kV199VVu2bNHhw4f1ox/9yN5/+vRp5ebmqr29Xdu2bdPy5ctVUVGhkpISe8z+/fuVm5urm266SY2NjZo9e7buvPNOvf766+E+JQAAEIViLMuy+vIFPvroI6WkpGjLli2aMGGCAoGA/uqv/korV67Uj3/8Y0nS3r17NWrUKNXV1en666/Xn//8Z9166606fPiw3G63JKm8vFz333+/PvroIyUkJOj+++9XZWWl3n33Xfu1pk6dqtbWVlVVVZ3T3ILBoFwulwKBgJxOZ1jPe0RxZViPBwAInwOLciM9BXwN5/r+3eefyQkEApKkIUOGSJIaGhrU0dGh7Oxse8zIkSN12WWXqa6uTpJUV1en0aNH24EjSTk5OQoGg2pqarLHnHmM7jHdx+hJW1ubgsFgyAMAAJgpvi8P3tXVpdmzZ2vcuHG6+uqrJUl+v18JCQlKTk4OGet2u+X3++0xZwZO9/7ufV82JhgM6tNPP1VSUtJZ8yktLdVDDz0UlnMDAESvaLzaztWn3uvTKzkFBQV69913tWrVqr58mXM2b948BQIB+3Ho0KFITwkAAPSRPruSU1hYqPXr16u2tlaXXnqpvd3j8ai9vV2tra0hV3NaWlrk8XjsMdu3bw85Xve3r84c8/lvZLW0tMjpdPZ4FUeSHA6HHA7H1z43AABw4Qv7lRzLslRYWKg1a9Zo06ZNSk9PD9k/duxYDRgwQDU1Nfa25uZmHTx4UD6fT5Lk8/m0a9cuHTlyxB5TXV0tp9OpjIwMe8yZx+ge030MAABwcQv7lZyCggKtXLlS//7v/65BgwbZn6FxuVxKSkqSy+VSfn6+ioqKNGTIEDmdTv3qV7+Sz+fT9ddfL0maOHGiMjIyNH36dJWVlcnv9+uBBx5QQUGBfSXmrrvu0gsvvKC5c+fq5z//uTZt2qRXXnlFlZXR93tWAAAQfmG/krN06VIFAgH9zd/8jVJTU+3H6tWr7THPPPOMbr31Vk2ZMkUTJkyQx+PRa6+9Zu+Pi4vT+vXrFRcXJ5/Pp5/+9Ke644479PDDD9tj0tPTVVlZqerqal1zzTV66qmn9NJLLyknJyfcpwQAAKJQn98n50LGfXIAANGCb1f9fxfMfXIAAAAigcgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABiJyAEAAEYicgAAgJGIHAAAYCQiBwAAGInIAQAARiJyAACAkYgcAABgJCIHAAAYicgBAABGInIAAICRiBwAAGAkIgcAABgpPtITAAAAX21EcWWkp9BrBxblRvT1uZIDAACMROQAAAAjRX3kLFmyRCNGjFBiYqKysrK0ffv2SE8JAABcAKI6clavXq2ioiItWLBAb7/9tq655hrl5OToyJEjkZ4aAACIsKiOnKefflozZ87Uz372M2VkZKi8vFyXXHKJfv/730d6agAAIMKi9ttV7e3tamho0Lx58+xtsbGxys7OVl1dXY/PaWtrU1tbm/1zIBCQJAWDwbDPr6vtk7AfEwCAaNIX769nHteyrC8dF7WR8/HHH+v06dNyu90h291ut/bu3dvjc0pLS/XQQw+dtT0tLa1P5ggAwMXM9WzfHv/48eNyuVxfuD9qI+d8zJs3T0VFRfbPXV1dOnr0qIYOHaqYmJjzPm4wGFRaWpoOHTokp9MZjqniC7DW/Ye17j+sdf9hrftPX661ZVk6fvy4vF7vl46L2sgZNmyY4uLi1NLSErK9paVFHo+nx+c4HA45HI6QbcnJyWGbk9Pp5F+afsJa9x/Wuv+w1v2Hte4/fbXWX3YFp1vUfvA4ISFBY8eOVU1Njb2tq6tLNTU18vl8EZwZAAC4EETtlRxJKioq0owZM5SZmanrrrtOzz77rE6ePKmf/exnkZ4aAACIsKiOnNtvv10fffSRSkpK5Pf7NWbMGFVVVZ31YeS+5nA4tGDBgrN+FYbwY637D2vdf1jr/sNa958LYa1jrK/6/hUAAEAUitrP5AAAAHwZIgcAABiJyAEAAEYicgAAgJGInHO0ZMkSjRgxQomJicrKytL27du/dPyrr76qkSNHKjExUaNHj9aGDRv6aabRrzdr/eKLL2r8+PEaPHiwBg8erOzs7K/8/wb/X2//ue62atUqxcTEaPLkyX07QYP0dq1bW1tVUFCg1NRUORwOfetb3+K/I+eot2v97LPP6qqrrlJSUpLS0tI0Z84cnTp1qp9mG71qa2t12223yev1KiYmRmvXrv3K52zevFnXXnutHA6HrrzySlVUVPTtJC18pVWrVlkJCQnW73//e6upqcmaOXOmlZycbLW0tPQ4fuvWrVZcXJxVVlZm7d6923rggQesAQMGWLt27ernmUef3q71tGnTrCVLllg7d+609uzZY/3TP/2T5XK5rA8//LCfZx59ervW3fbv32/99V//tTV+/Hjr7/7u7/pnslGut2vd1tZmZWZmWpMmTbLeeOMNa//+/dbmzZutxsbGfp559OntWq9YscJyOBzWihUrrP3791uvv/66lZqaas2ZM6efZx59NmzYYM2fP9967bXXLEnWmjVrvnT8vn37rEsuucQqKiqydu/ebT3//PNWXFycVVVV1WdzJHLOwXXXXWcVFBTYP58+fdryer1WaWlpj+N/8pOfWLm5uSHbsrKyrF/84hd9Ok8T9HatP6+zs9MaNGiQtXz58r6aojHOZ607OzutG264wXrppZesGTNmEDnnqLdrvXTpUuvyyy+32tvb+2uKxujtWhcUFFg/+MEPQrYVFRVZ48aN69N5muZcImfu3LnWt7/97ZBtt99+u5WTk9Nn8+LXVV+hvb1dDQ0Nys7OtrfFxsYqOztbdXV1PT6nrq4uZLwk5eTkfOF4fOZ81vrzPvnkE3V0dGjIkCF9NU0jnO9aP/zww0pJSVF+fn5/TNMI57PW69atk8/nU0FBgdxut66++mo9/vjjOn36dH9NOyqdz1rfcMMNamhosH+ltW/fPm3YsEGTJk3qlzlfTCLx3hjVdzzuDx9//LFOnz591l2U3W639u7d2+Nz/H5/j+P9fn+fzdME57PWn3f//ffL6/We9S8SQp3PWr/xxht6+eWX1djY2A8zNMf5rPW+ffu0adMm5eXlacOGDfrggw909913q6OjQwsWLOiPaUel81nradOm6eOPP9aNN94oy7LU2dmpu+66S7/5zW/6Y8oXlS96bwwGg/r000+VlJQU9tfkSg6MsWjRIq1atUpr1qxRYmJipKdjlOPHj2v69Ol68cUXNWzYsEhPx3hdXV1KSUnRsmXLNHbsWN1+++2aP3++ysvLIz0142zevFmPP/64fve73+ntt9/Wa6+9psrKSj3yyCORnhrCgCs5X2HYsGGKi4tTS0tLyPaWlhZ5PJ4en+PxeHo1Hp85n7Xu9uSTT2rRokX6j//4D33nO9/py2kaobdr/T//8z86cOCAbrvtNntbV1eXJCk+Pl7Nzc264oor+nbSUep8/rlOTU3VgAEDFBcXZ28bNWqU/H6/2tvblZCQ0Kdzjlbns9YPPvigpk+frjvvvFOSNHr0aJ08eVKzZs3S/PnzFRvLtYBw+aL3RqfT2SdXcSSu5HylhIQEjR07VjU1Nfa2rq4u1dTUyOfz9fgcn88XMl6Sqqurv3A8PnM+ay1JZWVleuSRR1RVVaXMzMz+mGrU6+1ajxw5Urt27VJjY6P9+Nu//VvddNNNamxsVFpaWn9OP6qczz/X48aN0wcffGCHpCS99957Sk1NJXC+xPms9SeffHJWyHTHpcWfdgyriLw39tlHmg2yatUqy+FwWBUVFdbu3butWbNmWcnJyZbf77csy7KmT59uFRcX2+O3bt1qxcfHW08++aS1Z88ea8GCBXyF/Bz1dq0XLVpkJSQkWP/2b/9m/d///Z/9OH78eKROIWr0dq0/j29XnbvervXBgwetQYMGWYWFhVZzc7O1fv16KyUlxXr00UcjdQpRo7drvWDBAmvQoEHWv/7rv1r79u2zNm7caF1xxRXWT37yk0idQtQ4fvy4tXPnTmvnzp2WJOvpp5+2du7caf3lL3+xLMuyiouLrenTp9vju79Cft9991l79uyxlixZwlfILxTPP/+8ddlll1kJCQnWddddZ7355pv2vu9///vWjBkzQsa/8sor1re+9S0rISHB+va3v21VVlb284yjV2/Wevjw4Zaksx4LFizo/4lHod7+c30mIqd3ervW27Zts7KysiyHw2Fdfvnl1mOPPWZ1dnb286yjU2/WuqOjw1q4cKF1xRVXWImJiVZaWpp19913W8eOHev/iUeZ//zP/+zxv7/d6ztjxgzr+9///lnPGTNmjJWQkGBdfvnl1h/+8Ic+nWOMZXE9DgAAmIfP5AAAACMROQAAwEhEDgAAMBKRAwAAjETkAAAAIxE5AADASEQOAAAwEpEDAACMROQAAAAjETkAAMBIRA4AADASkQMAAIz0/wAMqLyYO7DnygAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot label distribution\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change y into 1 and 0 so we can use it for binary classification\n",
    "# y < 0.60 = 0 (not a good sequence)\n",
    "# y >= 0.60 = 1 (good sequence)\n",
    "\n",
    "labels = [1 if y >= 0.60 else 0 for y in labels]\n",
    "\n",
    "plt.hist(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        modelpath,\n",
    "        \n",
    "        model_max_length=100,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  ['AAAAAAAAACTCCAAAACCCT', 'AAAAAACAACAAGAAGCACAA']\n",
      "Token IDs: tensor([[   1,  750, 2890,   13, 1038,    2,    3,    3,    3,    3],\n",
      "        [   1,  142,  361,  706,   27,    2,    3,    3,    3,    3]])\n",
      "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "#sequences = sequences.tolist()\n",
    "\n",
    "output = tokenizer(\n",
    "        sequences,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "import datasets\n",
    "\n",
    "#turn df into a dataset\n",
    "tokenized = datasets.Dataset.from_dict(output)\n",
    "\n",
    "\n",
    "\n",
    "# padded = torch.nn.utils.rnn.pad_sequence(\n",
    "#         output['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "#     )\n",
    "\n",
    "input_ids = output['input_ids']\n",
    "attention_masks = output['attention_mask']\n",
    "\n",
    "#Convert labels to tensors\n",
    "labels = torch.Tensor(labels).long()\n",
    "\n",
    "print('Original: ', sequences[:2])\n",
    "print('Token IDs:', input_ids[:2])\n",
    "print('Attention Mask:', attention_masks[:2])\n",
    "\n",
    "print(type(input_ids))\n",
    "print(type(attention_masks))\n",
    "print(type(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 56888\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'torch.Tensor'>\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 56888\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized)\n",
    "print(type(tokenized))\n",
    "\n",
    "\n",
    "print(type(tokenized['input_ids']))\n",
    "\n",
    "tokenized.set_format(\"torch\")\n",
    "\n",
    "\n",
    "print(tokenized)\n",
    "print(type(tokenized))\n",
    "\n",
    "\n",
    "print(type(tokenized['input_ids']))\n",
    "print(tokenized['input_ids'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([10])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m pos_shapes \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(pos_shapes\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(\u001b[43mtokenized\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[i]\u001b[38;5;241m.\u001b[39mshape)):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\arrow_dataset.py:2795\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2793\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2794\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2795\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\arrow_dataset.py:2780\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2778\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2779\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 2780\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2782\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2783\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\formatting\\formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\formatting\\formatting.py:398\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_batch(pa_table)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\formatting\\torch_formatter.py:93\u001b[0m, in \u001b[0;36mTorchFormatter.format_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     95\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecursive_tensorize(column)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\formatting\\formatting.py:161\u001b[0m, in \u001b[0;36mNumpyArrowExtractor.extract_column\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arrow_array_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\datasets\\formatting\\formatting.py:196\u001b[0m, in \u001b[0;36mNumpyArrowExtractor._arrow_array_to_numpy\u001b[1;34m(self, pa_array)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    191\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m array[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(x))\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m array\n\u001b[0;32m    194\u001b[0m     ):\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(array, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m)\n\u001b[1;32m--> 196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Check if padding works\n",
    "pos_shapes = []\n",
    "\n",
    "for i in range(len(tokenized['input_ids'])):\n",
    "    if(pos_shapes.__contains__(tokenized['input_ids'][i].shape)):\n",
    "        continue\n",
    "    else:\n",
    "        pos_shapes.append(tokenized['input_ids'][i].shape)\n",
    "\n",
    "print(pos_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51,199 training samples\n",
      "5,689 validation samples\n",
      "(tensor([   1,  553, 1413,  135,  192,    2,    3,    3,    3,    3]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )\n",
    "\n",
    "\n",
    "# for batch in train_dataloader:\n",
    "#     print(batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin/.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\1d020b803b871a976f5f3d5565f0eac8f2c7bb81\\bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    modelpath,\n",
    "    cache_dir=None,\n",
    "    num_labels=2,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,\n",
    "                  eps = 1e-8\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   1,    5, 3440,   45,  507,   72,    2,    3,    3,    3],\n",
      "        [   1,    5,  231,  725,  104, 3131,    2,    3,    3,    3],\n",
      "        [   1,  229, 2465,  332,   25,    2,    3,    3,    3,    3],\n",
      "        [   1,   66,   83,  245,  123,   42,    2,    3,    3,    3],\n",
      "        [   1, 3242,  114,   22,  359,   40,    2,    3,    3,    3],\n",
      "        [   1,    5,   83,  135,  103,  173, 1049,    2,    3,    3],\n",
      "        [   1,  229,  104, 1700,  330,    6,    2,    3,    3,    3],\n",
      "        [   1,  317,   73,   13,  962,   19,    7,    2,    3,    3]]), tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]), tensor([0, 1, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ones_like(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mloss, outputs\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\1d020b803b871a976f5f3d5565f0eac8f2c7bb81\\bert_layers.py:862\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    842\u001b[0m     input_ids: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;66;03m# (mean-square loss). If `config.num_labels > 1` a classification loss\u001b[39;00m\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# is computed (cross-entropy).\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 862\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    874\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    876\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\1d020b803b871a976f5f3d5565f0eac8f2c7bb81\\bert_layers.py:591\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, position_ids, output_all_encoded_layers, masked_tokens_mask, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    582\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[List[torch\u001b[38;5;241m.\u001b[39mTensor], torch\u001b[38;5;241m.\u001b[39mTensor], Optional[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_type_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    593\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_ids)\n",
      "\u001b[1;31mTypeError\u001b[0m: ones_like(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "outputs = model(batch)\n",
    "print(outputs.loss, outputs.logits.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
