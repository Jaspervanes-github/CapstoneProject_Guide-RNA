{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56888 entries, 0 to 56887\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   seq     56888 non-null  object \n",
      " 1   y       56888 non-null  float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 889.0+ KB\n",
      "None\n",
      "                     seq         y\n",
      "0  AAAAAAAAACTCCAAAACCCT  0.093147\n",
      "1  AAAAAACAACAAGAAGCACAA  0.064951\n",
      "2  AAAAAACACAAGCAAGACCGT  0.061797\n",
      "3  AAAAAACAGATGCCACCTGTG  0.057246\n",
      "4  AAAAAACCCGTAGATAGCCTC  0.067596\n"
     ]
    }
   ],
   "source": [
    "modelpath = \"zhihan1996/DNABERT-2-117M\"\n",
    "df = pd.read_csv(\"./sample_data/esp_decoded.csv\")\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plot label distribution\n",
    "plt.hist(df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "#turn df into a dataset\n",
    "raw_dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['seq', 'y'],\n",
      "    num_rows: 1560\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#raw_dataset = raw_dataset.select(range(int(1300 * 1.2)))\n",
    "\n",
    "print(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        modelpath,\n",
    "        \n",
    "        model_max_length=100,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1560/1560 [00:00<00:00, 33199.12 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['seq', 'y', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1560\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"seq\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "#print(output)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into train and validation set\n",
    "tokenized_datasets = tokenized_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['seq', 'y', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 1248\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['seq', 'y', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 312\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenized_datasets)\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"seq\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"y\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\", padding=\"longest\", max_length=tokenizer.model_max_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(\n",
    "#     tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    "# )\n",
    "# eval_dataloader = DataLoader(\n",
    "#     tokenized_datasets[\"test\"], batch_size=batch_size, collate_fn=data_collator\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin/.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\25abaf0bd247444fcfa837109f12088114898d98\\bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertUnpadAttention(\n",
       "            (self): BertUnpadSelfAttention(\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (Wqkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (mlp): BertGatedLinearUnitMLP(\n",
       "            (gated_layers): Linear(in_features=768, out_features=6144, bias=False)\n",
       "            (act): GELU(approximate='none')\n",
       "            (wo): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    modelpath,\n",
    "    cache_dir=None,\n",
    "    num_labels=1,\n",
    "    trust_remote_code=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     print(batch['labels'][0])\n",
    "#     break\n",
    "# print({k: v.shape for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.reshape(-1, 1)\n",
    "\n",
    "    logits = logits[0]\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\transformers\\optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 10/30 [00:49<01:39,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0409, 'learning_rate': 1.9999999999999998e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 33%|███▎      | 10/30 [00:49<01:39,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02351677045226097, 'eval_mse': 0.02351677045226097, 'eval_rmse': 0.1533517837524414, 'eval_mae': 0.12378602474927902, 'eval_r2': 0.014300998896091266, 'eval_runtime': 0.7307, 'eval_samples_per_second': 426.977, 'eval_steps_per_second': 13.685, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [01:08<01:37,  5.76s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 31\u001b[0m\n\u001b[0;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     22\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,                         \n\u001b[0;32m     23\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,                  \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m data_collator, \n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Call the summary\u001b[39;00m\n\u001b[0;32m     34\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\transformers\\trainer.py:1664\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1661\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1663\u001b[0m )\n\u001b[1;32m-> 1664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\robin\\anaconda3\\envs\\dnabert\\lib\\site-packages\\transformers\\trainer.py:1942\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1940\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1943\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1945\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1946\u001b[0m ):\n\u001b[0;32m   1947\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Specifiy the arguments for the trainer  \n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./results',          \n",
    "    num_train_epochs = 3,     \n",
    "    per_device_train_batch_size = 128,   \n",
    "    per_device_eval_batch_size = 32,   \n",
    "    weight_decay = 0.01,               \n",
    "    learning_rate = 3e-5,\n",
    "    logging_dir = './logs',\n",
    "    logging_steps = 10,        \n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,     \n",
    "    metric_for_best_model = 'rmse',    \n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    ") \n",
    "\n",
    "# Call the Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,                         \n",
    "    args = training_args,                  \n",
    "    train_dataset = tokenized_datasets[\"train\"],         \n",
    "    eval_dataset = tokenized_datasets[\"test\"],          \n",
    "    compute_metrics = compute_metrics,\n",
    "    data_collator = data_collator, \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Call the summary\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\robin/.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\1d020b803b871a976f5f3d5565f0eac8f2c7bb81\\bert_layers.py:125: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at zhihan1996/DNABERT-2-117M were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['classifier.weight', 'classifier.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0495],\n",
      "        [ 0.0050],\n",
      "        [ 0.0346],\n",
      "        [ 0.1536],\n",
      "        [-0.0072]])\n",
      "tensor([[0.6034],\n",
      "        [0.5854],\n",
      "        [0.7205],\n",
      "        [0.1395],\n",
      "        [0.5100]])\n",
      "mse:  0.2381744  rmse:  0.48803115  mae:  0.440737  r2:  -4.579909511011868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    modelpath,\n",
    "    cache_dir=None,\n",
    "    num_labels=1,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "#metric = evaluate.load(\"accuracy\")\n",
    "#model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    labels = batch[\"labels\"].reshape(-1, 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    #calculate metrics\n",
    "\n",
    "    logits = outputs[\"logits\"]\n",
    "    \n",
    "    #use cpu to calculate metrics\n",
    "    logits = logits.cpu()\n",
    "    labels = labels.cpu()\n",
    "\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "    #smape = 1/len(labels) * np.sum(2 * np.abs(logits-labels) / (np.abs(labels) + np.abs(logits))*100)\n",
    "    print(logits[:5])\n",
    "    print(labels[:5])\n",
    "\n",
    "    print(\"mse: \", mse, \" rmse: \", rmse, \" mae: \", mae, \" r2: \", r2)\n",
    "    break\n",
    "\n",
    "\n",
    "#metric.compute()\n",
    "\n",
    "#trained\n",
    "\n",
    "# tensor([[0.5489],\n",
    "#         [0.3944],\n",
    "#         [0.7925],\n",
    "#         [0.4906],\n",
    "#         [0.5423]])\n",
    "# tensor([[0.6034],\n",
    "#         [0.5854],\n",
    "#         [0.7205],\n",
    "#         [0.1395],\n",
    "#         [0.5100]])\n",
    "# mse:  0.031777926  rmse:  0.17826363  mae:  0.13696145  r2:  0.25551206762590795\n",
    "\n",
    "#untrained\n",
    "\n",
    "# tensor([[-0.0495],\n",
    "#         [ 0.0050],\n",
    "#         [ 0.0346],\n",
    "#         [ 0.1536],\n",
    "#         [-0.0072]])\n",
    "# tensor([[0.6034],\n",
    "#         [0.5854],\n",
    "#         [0.7205],\n",
    "#         [0.1395],\n",
    "#         [0.5100]])\n",
    "# mse:  0.2381744  rmse:  0.48803115  mae:  0.440737  r2:  -4.579909511011868"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnabert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
